{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPORT\n",
    "\n",
    "1. Data description\n",
    "\n",
    "This is a binary classification dataset for stroke prediction, consisting of 5110 records and 12 features that are related to stroke risk factors. The 10 features include:\n",
    "\n",
    "Demographics: Gender, age, marital status, residence type\n",
    "\n",
    "Medical History: Hypertension, heart disease\n",
    "\n",
    "Lifestyle Factors: Work type, smoking status\n",
    "\n",
    "Health Metrics: Average glucose level, BMI\n",
    "\n",
    "The target variable is stroke occurrence (binary classification). There are missing values in the BMI column which we can use median imputation to resolve. Feature engineering is used to add more factors for effective learning.\n",
    "\n",
    "2. The objectives\n",
    "\n",
    "The goal of the analysis is to create a supervised learning model that predicts whether or not an individual has a stroke based on various risk factors. \n",
    "-We want the model to not overfit and generalize well to new examples. \n",
    "-We want a high accuracy which is crucial for health-related tasks.\n",
    "\n",
    "3.\n",
    "\n",
    "Two deep learning architectures were tested:\n",
    "\n",
    "Complex model: A deep network with four layers of 16 neurons each. This model showed overfitting, with significantly lower validation accuracy compared to training accuracy.\n",
    "\n",
    "Optimized / Simplified model: A simpler architecture with one hidden layer containing 8 neurons, ReLU activation, and L2 regularization. This model performed better in terms of validation accuracy and generalization.\n",
    "\n",
    "I selected the simplified model, which achieved stable validation accuracy (~95%) within 20 epochs.\n",
    "\n",
    "4. Key Findings\n",
    "\n",
    "-The optimized model achieved a validation accuracy of ~95%, with minimal overfitting.\n",
    "\n",
    "-The model converged quickly, suggesting that the dataset is relatively small for deep learning applications.\n",
    "\n",
    "-Feature importance analysis and correlation tests indicate that age and average glucose level have strong correlations with stroke occurrence\n",
    "\n",
    "5. Model Limitations and Future Improvements\n",
    "\n",
    "-The dataset likely has an imbalance in stroke occurrences, which could affect model learning. Using SMOTE or class-weight adjustments may help.\n",
    "-Trying ensemble methods, such as boosting or hybrid deep learning approaches, could be beneficial.\n",
    "-The insufficient number of training examples may limit the model's capabilities to genearalize to new data. Collecting additional records or synthetic data generation might resolve this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Loading the data - Stroke Prediction Dataset from Kaggle\n",
    "file_path = \"Stroke Prediction Dataset - FEDESORIANO.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Data processing and feature engineering\n",
    "\n",
    "#ID column is not needed. Create new features before splitting into X and y\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "#Create age groups\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 20, 40, 60, 80, 100], labels=['0-20', '21-40', '41-60', '61-80', '80+'])\n",
    "\n",
    "#Create BMI categories based on WHO standards\n",
    "df['bmi_category'] = pd.cut(df['bmi'], \n",
    "                           bins=[0, 18.5, 24.9, 29.9, float('inf')],\n",
    "                           labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "\n",
    "#Create glucose level categories\n",
    "df['glucose_category'] = pd.cut(df['avg_glucose_level'],\n",
    "                               bins=[0, 70, 100, 125, float('inf')],\n",
    "                               labels=['Low', 'Normal', 'Pre-diabetic', 'Diabetic'])\n",
    "\n",
    "#Create health risk score combining hypertension and heart disease\n",
    "df['health_risk'] = df['hypertension'] + df['heart_disease']\n",
    "\n",
    "#Split features and target\n",
    "X = df.drop(columns=[\"stroke\"])\n",
    "y = df[\"stroke\"]\n",
    "\n",
    "cat_cols = [\"gender\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\", \n",
    "            \"age_group\", \"bmi_category\", \"glucose_category\"]\n",
    "num_cols = [\"age\", \"hypertension\", \"heart_disease\", \"avg_glucose_level\", \"bmi\", \"health_risk\"]\n",
    "\n",
    "#Processed based on variable type - One-hot encoding for categorical, filling in missing values and normalization for numerical\n",
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "num_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "    (\"scaler\", StandardScaler())  \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", num_transformer, num_cols),\n",
    "    (\"cat\", cat_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "#0.75-0.25 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 1s 938us/step - loss: 0.6744 - accuracy: 0.6033 - val_loss: 0.6213 - val_accuracy: 0.6729\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 487us/step - loss: 0.5756 - accuracy: 0.7419 - val_loss: 0.5369 - val_accuracy: 0.7825\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 477us/step - loss: 0.4996 - accuracy: 0.8288 - val_loss: 0.4712 - val_accuracy: 0.8568\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 474us/step - loss: 0.4397 - accuracy: 0.8862 - val_loss: 0.4188 - val_accuracy: 0.9014\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 486us/step - loss: 0.3919 - accuracy: 0.9165 - val_loss: 0.3762 - val_accuracy: 0.9288\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 496us/step - loss: 0.3532 - accuracy: 0.9329 - val_loss: 0.3413 - val_accuracy: 0.9366\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 497us/step - loss: 0.3216 - accuracy: 0.9423 - val_loss: 0.3128 - val_accuracy: 0.9437\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 490us/step - loss: 0.2956 - accuracy: 0.9470 - val_loss: 0.2894 - val_accuracy: 0.9476\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 489us/step - loss: 0.2743 - accuracy: 0.9504 - val_loss: 0.2702 - val_accuracy: 0.9491\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 474us/step - loss: 0.2568 - accuracy: 0.9512 - val_loss: 0.2542 - val_accuracy: 0.9499\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 472us/step - loss: 0.2424 - accuracy: 0.9515 - val_loss: 0.2413 - val_accuracy: 0.9507\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 480us/step - loss: 0.2306 - accuracy: 0.9515 - val_loss: 0.2307 - val_accuracy: 0.9507\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 482us/step - loss: 0.2209 - accuracy: 0.9515 - val_loss: 0.2220 - val_accuracy: 0.9515\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 594us/step - loss: 0.2129 - accuracy: 0.9515 - val_loss: 0.2148 - val_accuracy: 0.9515\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 506us/step - loss: 0.2064 - accuracy: 0.9515 - val_loss: 0.2089 - val_accuracy: 0.9515\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 504us/step - loss: 0.2010 - accuracy: 0.9515 - val_loss: 0.2042 - val_accuracy: 0.9515\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 493us/step - loss: 0.1966 - accuracy: 0.9512 - val_loss: 0.2004 - val_accuracy: 0.9515\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 494us/step - loss: 0.1930 - accuracy: 0.9512 - val_loss: 0.1972 - val_accuracy: 0.9515\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 498us/step - loss: 0.1899 - accuracy: 0.9512 - val_loss: 0.1946 - val_accuracy: 0.9515\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 486us/step - loss: 0.1874 - accuracy: 0.9512 - val_loss: 0.1925 - val_accuracy: 0.9515\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, regularizers\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "#First started with a deep network with 4 16-neuron layers,\n",
    "#caused overfitting as reflected by low validation accuracy compared to training accuracy.\n",
    "#Switched to a simpler network with 1 8-neuron layer and ReLU activation and L2 regularization. \n",
    "#Sigmoid function applied in the final layer as this is a binary classification task.\n",
    "\n",
    "model = keras.Sequential([\n",
    "    \n",
    "    layers.Dense(8, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "#Adam optimizer and binary crossentropy loss function achieved the highest accuracy.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#Train model. The model converges very quickly at around 20 epochs, fluctuating around 0.95 validation accuracy. Small difference between training and validation accuracy indicates the model has generalized well for this task. \n",
    "# The small amount of epochs needed for convergence may also indicate insufficient data. Larger datasets, further feature engineering or generating synthetic data using SMOTE may be needed.\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), \n",
    "                    verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
